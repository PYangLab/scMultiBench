{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02664e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from scipy.spatial import distance_matrix\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy as sp\n",
    "import math\n",
    "methods=['spiral','harmony_SEDR','harmony_STAGATE','seurat','harmony','DeepST','STAligner','GraphST','BASS']\n",
    "methods1=['SPIRAL','harmony_SEDR','harmony_STAGATE','seurat','harmony','DeepST','STAligner','GraphST','BASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47c3f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(adata, degree = 4):\n",
    "        \"\"\"\n",
    "        Converts spatial coordinates into graph using networkx library.\n",
    "        \n",
    "        param: adata - ST Slice \n",
    "        param: degree - number of edges per vertex\n",
    "\n",
    "        return: 1) G - networkx graph\n",
    "                2) node_dict - dictionary mapping nodes to spots\n",
    "        \"\"\"\n",
    "        D = distance_matrix(adata.obsm['spatial'], adata.obsm['spatial'])\n",
    "        # Get column indexes of the degree+1 lowest values per row\n",
    "        idx = np.argsort(D, 1)[:, 0:degree+1]\n",
    "        # Remove first column since it results in self loops\n",
    "        idx = idx[:, 1:]\n",
    "\n",
    "        G = nx.Graph()\n",
    "        for r in range(len(idx)):\n",
    "            for c in idx[r]:\n",
    "                G.add_edge(r, c)\n",
    "\n",
    "        node_dict = dict(zip(range(adata.shape[0]), adata.obs.index))\n",
    "        return G, node_dict\n",
    "    \n",
    "def generate_graph_from_labels(adata, labels_dict,knn):\n",
    "    \"\"\"\n",
    "    Creates and returns the graph and dictionary {node: cluster_label} for specified layer\n",
    "    \"\"\"\n",
    "    \n",
    "    g, node_to_spot = create_graph(adata,knn)\n",
    "    spot_to_cluster = labels_dict\n",
    "\n",
    "    # remove any nodes that are not mapped to a cluster\n",
    "    removed_nodes = []\n",
    "    for node in node_to_spot.keys():\n",
    "        if (node_to_spot[node] not in spot_to_cluster.keys()):\n",
    "            removed_nodes.append(node)\n",
    "\n",
    "    for node in removed_nodes:\n",
    "        del node_to_spot[node]\n",
    "        g.remove_node(node)\n",
    "        \n",
    "    labels = dict(zip(g.nodes(), [spot_to_cluster[node_to_spot[node]] for node in g.nodes()]))\n",
    "    return g, labels\n",
    "\n",
    "def spatial_coherence_score(graph, labels):\n",
    "    g, l = graph, labels\n",
    "    true_entropy = spatial_entropy(g, l)\n",
    "    entropies = []\n",
    "    for i in range(1000):\n",
    "        new_l = list(l.values())\n",
    "        random.shuffle(new_l)\n",
    "        labels = dict(zip(l.keys(), new_l))\n",
    "        entropies.append(spatial_entropy(g, labels))\n",
    "        \n",
    "    return (true_entropy - np.mean(entropies))/np.std(entropies)\n",
    "\n",
    "def spatial_entropy(g, labels):\n",
    "    \"\"\"\n",
    "    Calculates spatial entropy of graph  \n",
    "    \"\"\"\n",
    "    # construct contiguity matrix C which counts pairs of cluster edges\n",
    "    cluster_names = np.unique(list(labels.values()))\n",
    "    C = pd.DataFrame(0,index=cluster_names, columns=cluster_names)\n",
    "\n",
    "    for e in g.edges():\n",
    "        C[labels[e[0]]][labels[e[1]]] += 1\n",
    "\n",
    "    # calculate entropy from C\n",
    "    C_sum = C.values.sum()\n",
    "    H = 0\n",
    "    for i in range(len(cluster_names)):\n",
    "        for j in range(i, len(cluster_names)):\n",
    "            if (i == j):\n",
    "                z = C[cluster_names[i]][cluster_names[j]]\n",
    "            else:\n",
    "                z = C[cluster_names[i]][cluster_names[j]] + C[cluster_names[j]][cluster_names[i]]\n",
    "            if z != 0:\n",
    "                H += -(z/C_sum)*math.log(z/C_sum)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a03111",
   "metadata": {},
   "source": [
    "输入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bf896ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs=\"/data02/tguo/space_batch_effect/mouse_brain/\"\n",
    "# # sample_name=[\"all_posterior1\",\"all_posterior2\",\"all_anterior1\",\"all_anterior2\"]\n",
    "sample_name=[\"posterior1\",\"posterior2\"]\n",
    "# sample_name=[\"anterior1\",\"anterior2\"]\n",
    "# dirs=\"/data02/tguo/space_batch_effect/Hippo/\"\n",
    "# sample_name=['10X_Normal','10X_DAPI','10X_FFPE']\n",
    "i=0\n",
    "feat=pd.read_csv(dirs+\"gtt_input/\"+str(sample_name[i])+\"_mat.csv\",header=0,index_col=0,sep=',')\n",
    "meta=pd.read_csv(dirs+\"gtt_input/\"+str(sample_name[i])+\"_meta.csv\",header=0,index_col=0,sep=',')\n",
    "coord=pd.read_csv(dirs+\"gtt_input/\"+str(sample_name[i])+\"_coord.csv\",header=0,index_col=0,sep=',')\n",
    "flags='_'+str(sample_name[i])\n",
    "for sample in np.array(sample_name)[1:len(sample_name)]:\n",
    "    feat=pd.concat((feat,pd.read_csv(dirs+\"gtt_input/\"+str(sample)+\"_mat.csv\",header=0,index_col=0,sep=',')))\n",
    "    meta=pd.concat((meta,pd.read_csv(dirs+\"gtt_input/\"+str(sample)+\"_meta.csv\",header=0,index_col=0,sep=',')))\n",
    "    coord=pd.concat((coord,pd.read_csv(dirs+\"gtt_input/\"+str(sample)+\"_coord.csv\",header=0,index_col=0,sep=',')))\n",
    "    flags=flags+'_'+str(sample)\n",
    "# meta.loc[:,'celltype'][meta.loc[:,'celltype']=='AC']='CA'\n",
    "adata = sc.AnnData(feat)\n",
    "adata.var_names_make_unique()\n",
    "coord = coord.loc[adata.obs_names, ['x', 'y']]\n",
    "adata.obsm[\"spatial\"] = coord.to_numpy()\n",
    "adata.X=sp.sparse.csr_matrix(adata.X)\n",
    "adata.obs= meta.loc[adata.obs_names, :]   \n",
    "adata.obs['batch']=np.array(adata.obs['batch'],dtype=str)\n",
    "idx=np.where(adata.obs['celltype']!='Low_Quality')[0]\n",
    "adata=adata[idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fefa0e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirs=\"/data02/tguo/space_batch_effect/mouse_OB/\"\n",
    "# sample_name=[\"BGI\",\"SlideV2\"]\n",
    "# IDX=np.arange(len(sample_name))\n",
    "# i=0\n",
    "# feat=pd.read_csv(dirs+\"gtt_input/BGI_SlideV2_10X/\"+str(sample_name[i])+\"_mat.csv\",header=0,index_col=0,sep=',')\n",
    "# meta=pd.read_csv(dirs+\"gtt_input/BGI_SlideV2_10X/\"+str(sample_name[i])+\"_meta.csv\",header=0,index_col=0,sep=',')\n",
    "# coord=pd.read_csv(dirs+\"gtt_input/BGI_SlideV2_10X/\"+str(sample_name[i])+\"_coord.csv\",header=0,index_col=0,sep=',')\n",
    "# flags='_'+str(sample_name[i])\n",
    "# for sample in np.array(sample_name)[1:len(sample_name)]:\n",
    "#     feat1=pd.read_csv(dirs+\"gtt_input/BGI_SlideV2_10X/\"+str(sample)+\"_mat.csv\",header=0,index_col=0,sep=',')\n",
    "#     meta1=pd.read_csv(dirs+\"gtt_input/BGI_SlideV2_10X/\"+str(sample)+\"_meta.csv\",header=0,index_col=0,sep=',')\n",
    "#     coord1=pd.read_csv(dirs+\"gtt_input/BGI_SlideV2_10X/\"+str(sample)+\"_coord.csv\",header=0,index_col=0,sep=',')\n",
    "#     if sample==\"SlideV2\":\n",
    "#         used_barcodes=np.loadtxt(dirs+\"origin/used_barcodes.txt\",dtype=str)\n",
    "#         used_barcodes=['SlideV2-'+x for x in used_barcodes]\n",
    "#         cells=np.intersect1d(used_barcodes,feat1.index)\n",
    "#         feat1=feat1.loc[cells,:]\n",
    "#         meta1=meta1.loc[cells,:]\n",
    "#         coord1=coord1.loc[cells,:]\n",
    "#     feat=pd.concat((feat,feat1))\n",
    "#     meta=pd.concat((meta,meta1))\n",
    "#     coord=pd.concat((coord,coord1))\n",
    "#     flags=flags+'_'+str(sample)\n",
    "    \n",
    "# adata = sc.AnnData(feat)\n",
    "# adata.var_names_make_unique()\n",
    "# coord = coord.loc[adata.obs_names, ['x', 'y']]\n",
    "# adata.obsm[\"spatial\"] = coord.to_numpy()\n",
    "# adata.X=sp.sparse.csr_matrix(adata.X)\n",
    "# adata.obs= meta.loc[adata.obs_names, :]   \n",
    "# adata.obs['batch']=np.array(adata.obs['batch'],dtype=str)\n",
    "# idx=np.where(adata.obs['celltype']!='Low_Quality')[0]\n",
    "# adata=adata[idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dab53451",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs=\"/data02/tguo/space_batch_effect/human_DLPFC_10x/\"\n",
    "sample_name=[151507,151508,151509,151510,151669,151670,151671,151672,151673,151674,151675,151676]\n",
    "IDX=[9,10]\n",
    "flags1=str(sample_name[IDX[0]])\n",
    "for i in np.arange(1,len(IDX)):\n",
    "    flags1=flags1+'-'+str(sample_name[IDX[i]])\n",
    "    \n",
    "i=IDX[0]\n",
    "feat=pd.read_csv(dirs+\"gtt_input_scanpy/\"+flags1+'_'+str(sample_name[i])+\"_features.txt\",header=0,index_col=0,sep=',')\n",
    "meta=pd.read_csv(dirs+\"gtt_input_scanpy/\"+flags1+'_'+str(sample_name[i])+\"_label.txt\",header=0,index_col=0,sep=',')\n",
    "coord=pd.read_csv(dirs+\"gtt_input_scanpy/\"+flags1+'_'+str(sample_name[i])+\"_positions.txt\",header=0,index_col=0,sep=',')\n",
    "flags='_'+str(sample_name[i])\n",
    "for sample in np.array(sample_name)[IDX[1:len(IDX)]]:\n",
    "    feat=pd.concat((feat,pd.read_csv(dirs+\"gtt_input_scanpy/\"+flags1+'_'+str(sample)+\"_features.txt\",header=0,index_col=0,sep=',')))\n",
    "    meta=pd.concat((meta,pd.read_csv(dirs+\"gtt_input_scanpy/\"+flags1+'_'+str(sample)+\"_label.txt\",header=0,index_col=0,sep=',')))\n",
    "    coord=pd.concat((coord,pd.read_csv(dirs+\"gtt_input_scanpy/\"+flags1+'_'+str(sample)+\"_positions.txt\",header=0,index_col=0,sep=',')))\n",
    "    flags=flags+'_'+str(sample)\n",
    "adata = sc.AnnData(feat)\n",
    "adata.var_names_make_unique()\n",
    "coord = coord.loc[adata.obs_names, ['x', 'y']]\n",
    "adata.obsm[\"spatial\"] = coord.to_numpy()\n",
    "adata.X=sp.sparse.csr_matrix(adata.X)\n",
    "adata.obs= meta.loc[adata.obs_names, :]   \n",
    "adata.obs['batch']=np.array(adata.obs['batch'],dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91a74f0",
   "metadata": {},
   "source": [
    "聚类的类别的SCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0d8339",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "clust_cate=['seuratmethod','louvain','louvain','louvain','louvain','louvain','louvain','louvain']\n",
    "BS=['_refine_512','','','','','','','']\n",
    "clust_cate=['mclust']*8\n",
    "BS=['','','','','','','','']\n",
    "for method in methods[1:8]:\n",
    "    clust=pd.read_csv(dirs+\"metrics/\"+method+flags+\"_\"+clust_cate[k]+\"_clust_modify\"+BS[k]+\".csv\",header=0,index_col=0,sep=',').loc[adata.obs_names,]\n",
    "    adata.obs[method+'_clust']=np.array(clust.values,dtype=str)\n",
    "    k+=1\n",
    "clust=pd.read_csv(dirs+'metrics/BASS_BASS_clust'+flags+'.csv',index_col=0,header=0)\n",
    "adata.obs['BASS_clust']=np.array(clust.loc[adata.obs_names,:].values,dtype=str)    \n",
    "knn=6\n",
    "ub=np.unique(adata.obs['batch'])\n",
    "scs=[]\n",
    "for i in np.arange(len(ub)):\n",
    "    idx=np.where(adata.obs['batch']==ub[i])[0]\n",
    "    adata1=adata[idx,:]\n",
    "    if ub[i]=='10X':\n",
    "        knn=6\n",
    "    for method in methods:\n",
    "        g,node_dict=generate_graph_from_labels(adata1, adata1.obs[method+'_clust'],knn)\n",
    "        scs.append(spatial_coherence_score(g, node_dict))\n",
    "    \n",
    "a1=pd.DataFrame(scs,columns=['scs'])\n",
    "a2=pd.DataFrame(methods1*len(ub),columns=['method'])\n",
    "data1=pd.concat((a1,a2),axis=1)\n",
    "data.to_csv(dirs+'metrics/spatial_coherence_score_louvain'+flags+\"_clusters_modify.csv\")\n",
    "data.to_csv(dirs+'metrics/spatial_coherence_score_'+clust_cate[0]+'-'+clust_cate[1]+flags+\"_clusters_modify.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d181a7",
   "metadata": {},
   "source": [
    "alignment后的scs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f68d98e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "######mouse OB########\n",
    "# clust_cate='_louvain'\n",
    "# coord1=pd.read_csv(dirs+\"gtt_output/coordinate_file/gtt_new_coordinate\"+flags+clust_cate+\".csv\",header=0,index_col=0)\n",
    "# coord2=pd.read_csv(dirs+\"methods/paste_new_coord\"+flags+\".csv\",header=0,index_col=0)\n",
    "# cells=np.array(coord1.index[np.where((coord1.loc[:,\"celltype\"]!='UN')&(coord1.loc[:,\"celltype\"]!='AOB')&(coord1.loc[:,\"celltype\"]!='AOBgr'))])\n",
    "# cells=np.intersect1d(cells,adata.obs_names)\n",
    "# adata=adata[cells,]\n",
    "# coord1=coord1.loc[cells,:]\n",
    "# coord2=coord2.loc[cells,:]\n",
    "# idx=np.where((coord1.loc[:,'celltype']=='GL_1')|(coord1.loc[:,'celltype']=='GL_2'))[0]\n",
    "# coord1.iloc[idx,3]='GL'\n",
    "# adata.obs['celltype']=coord1.loc[:,'celltype']\n",
    "\n",
    "#####sagittal#######\n",
    "for angle in [30,90,120,150,180]:\n",
    "    coord1=pd.read_csv(dirs+\"gtt_output/SPIRAL_alignment/new_coord\"+flags+\"_rotate\"+str(angle)+\"_modify.csv\",header=0,index_col=0)\n",
    "    coord2=pd.read_csv(dirs+\"methods/paste_new_coord\"+flags+\"_rotate\"+str(angle)+\".csv\",header=0,index_col=0)\n",
    "\n",
    "    coord1=coord1.loc[adata.obs_names, ['x', 'y']]\n",
    "    adata.obsm[\"spatial\"]=coord1.to_numpy()\n",
    "    knn=6\n",
    "    g1,node_dict1=generate_graph_from_labels(adata, adata.obs['celltype'],knn)\n",
    "    scs1=spatial_coherence_score(g1, node_dict1)\n",
    "\n",
    "    coord2=coord2.loc[adata.obs_names, ['x', 'y']]\n",
    "    adata.obsm[\"spatial\"]=coord2.to_numpy()\n",
    "    knn=6\n",
    "    g2,node_dict2=generate_graph_from_labels(adata, adata.obs['celltype'],knn)\n",
    "    scs2=spatial_coherence_score(g2, node_dict2)\n",
    "    scs=np.array([np.abs(scs1),np.abs(scs2)]).reshape(2,1)\n",
    "    method=np.array(['SPIRAL','PASTE']).reshape(2,1)\n",
    "    data=np.hstack((method,scs))\n",
    "    pd.DataFrame(data,columns=['method','scs']).to_csv(dirs+'metrics/spatial_coherence_score_louvain'+flags+\"_celltype_rotate\"+str(angle)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5bee69fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "coord3=pd.read_csv(dirs+\"methods/STIM_new_coord\"+flags+\".csv\",header=0,index_col=0).loc[adata.obs_names,]\n",
    "adata.obsm[\"spatial\"]=coord3.to_numpy()\n",
    "\n",
    "knn=6\n",
    "g1,node_dict1=generate_graph_from_labels(adata, adata.obs['celltype'],knn)\n",
    "scs1=spatial_coherence_score(g1, node_dict1)\n",
    "\n",
    "clust_cate='louvain'\n",
    "scs=pd.read_csv(dirs+'metrics/spatial_coherence_score_'+clust_cate+flags+\"_celltype.csv\",header=0,index_col=0)\n",
    "scs=pd.concat((scs,pd.DataFrame(np.array([['STIM',-scs1]]),columns=['method','scs'])))\n",
    "scs.iloc[0,0]='SPIRAL'\n",
    "scs.to_csv(dirs+'metrics/spatial_coherence_score_'+clust_cate+flags+\"_celltype.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34eed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "scs=np.array([np.abs(scs1),np.abs(scs2)]).reshape(-1,1)\n",
    "method=np.array(['GraphSCIDRL','PASTE']).reshape(-1,1)\n",
    "data=np.hstack((method,scs))\n",
    "pd.DataFrame(data,columns=['method','scs']).to_csv(dirs+'metrics/spatial_coherence_score_'+clust_cate+flags+\"_celltype.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
