For methods that utilise their own classifiers as described in their respective papers, the classification process varies across approaches. Multigrate trains a random forest classifier, following the implementation in their paper. Both sciCAN and SMILE use support vector machines (SVM) for classification. Several methods rely on k-nearest neighbors (kNN) with different values of k: GLUE uses k=15, while Seurat v5, scJoint, and SCALEX use k=30. In contrast, MultiMap, Concerto, and StabMap adopt a smaller k=5, tailored to their respective designs. For the MLP, as well as the SVM, random forest, and kNN methods, we provide examples in this folder.

Other methods utilise package-specific functions for classification. Seurat v3 applies the TransferData function, while sciPENN uses the predict function in its package. UnitedNet employs the predict_label function from its model for inference, and scBridge uses the infer_result function in eval_utils after training. Similarly, uniPort leverages the metrics.label_transfer function for label transfer, and Portal applies the annotate_by_nn function following integration. Lastly, Conos utilises the propagateLabels function to perform label transfer. This summary highlights the diverse strategies used for classification across methods with built-in classifiers. For these built-in classifiers, we provide demonstrations in the tools_scripts folder.
