{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T05:16:24.716518Z",
     "start_time": "2022-04-08T05:16:24.712532Z"
    }
   },
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-27T00:32:27.817639Z",
     "start_time": "2022-03-27T00:32:27.810657Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-10 20:16:36.473584: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "/home/lcl/mambaforge/envs/eggplant/lib/python3.8/site-packages/ot/backend.py:2998: UserWarning: To use TensorflowBackend, you need to activate the tensorflow numpy API. You can activate it by running: \n",
      "from tensorflow.python.ops.numpy_ops import np_config\n",
      "np_config.enable_numpy_behavior()\n",
      "  register_backend(TensorflowBackend())\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import style\n",
    "import paste as pst\n",
    "import ot\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from re import S\n",
    "import scanpy as sc\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import networkx as nx\n",
    "from scipy.spatial import distance_matrix\n",
    "import random\n",
    "import sklearn.metrics\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import scanpy as sc\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data and create AnnData object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_slices_h5(data_dir, file_names):\n",
    "    slices = []\n",
    "    for file_name in file_names:\n",
    "        slice_i = sc.read_h5ad(data_dir + file_name)\n",
    "        if scipy.sparse.issparse(slice_i.X):\n",
    "            slice_i.X = slice_i.X.toarray()\n",
    "\n",
    "        n_counts = slice_i.obs['n_counts']\n",
    "        layer_guess_reordered = slice_i.obs['layer_guess_reordered']\n",
    "        slice_i.obsm['spatial'] = slice_i.obs[['imagerow', 'imagecol']].to_numpy()\n",
    "        slice_i.obs = pd.DataFrame({'n_counts': n_counts, 'layer_guess_reordered': layer_guess_reordered})\n",
    "        slice_i.var = pd.DataFrame({'n_counts': slice_i.var['n_counts']})\n",
    "        sc.pp.filter_genes(slice_i, min_counts = 15)\n",
    "        sc.pp.filter_cells(slice_i, min_counts = 100)\n",
    "        slices.append(slice_i)\n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class visualize:\n",
    "    def __init__(self,slices,slice_colors):\n",
    "        self.slices = slices\n",
    "        self.slice_colors = slice_colors\n",
    "        self.center_color = 'orange'\n",
    "    def spatial_coordinates(self):\n",
    "        num_slices = len(self.slices)\n",
    "        num_rows = int(num_slices**0.5)\n",
    "        num_cols = int(num_slices / num_rows) + (num_slices % num_rows > 0)\n",
    "        \n",
    "        fig, axs = plt.subplots(num_rows, num_cols, figsize=(7, 7)) \n",
    "        axs = axs.flatten() \n",
    "        \n",
    "        for i, (slice_i, color) in enumerate(zip(self.slices, self.slice_colors)):\n",
    "            pst.plot_slice(slice_i, color, ax=axs[i])\n",
    "        \n",
    "        # If there are any empty subplots, hide them\n",
    "        for j in range(i + 1, len(axs)):\n",
    "            axs[j].axis('off')  \n",
    "        plt.tight_layout()\n",
    "        plt.show()  \n",
    "    def spatial_counts(self):\n",
    "        for slice_i in slices:    \n",
    "            sc.pl.spatial(slice_i, color=\"n_counts\", spot_size=1)\n",
    "\n",
    "    def new_slice(self,new_slices):\n",
    "        for i in range(len(new_slices)):\n",
    "            pst.plot_slice(new_slices[i],self.slice_colors[i],s=400)\n",
    "        plt.legend(handles=[mpatches.Patch(color=self.slice_colors[0], label='1'),mpatches.Patch(color=self.slice_colors[1], label='2'),mpatches.Patch(color=self.slice_colors[2], label='3'),mpatches.Patch(color=slice_colors[3], label='4')])\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_pairwise_layers(self,new_slices):\n",
    "        num_slices = len(new_slices)\n",
    "        num_plots = num_slices - 1\n",
    "\n",
    "        num_rows = num_plots // 2 + num_plots % 2\n",
    "        num_cols = 2\n",
    "        fig, axs = plt.subplots(num_rows, num_cols, figsize=(7, 7))\n",
    "        axs = axs.flatten() if num_plots > 1 else [axs]\n",
    "\n",
    "        for i in range(num_plots):\n",
    "            pst.plot_slice(new_slices[i], self.slice_colors[i % len(self.slice_colors)], ax=axs[i])\n",
    "            pst.plot_slice(new_slices[i + 1], self.slice_colors[(i + 1) % len(self.slice_colors)], ax=axs[i])\n",
    "\n",
    "        for j in range(num_plots, len(axs)):\n",
    "            fig.delaxes(axs[j])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def new_slices_3d(self,new_slices):\n",
    "        pio.renderers.default='notebook'\n",
    "        z_scale = 2\n",
    "\n",
    "        values = []\n",
    "        for i,L in enumerate(new_slices):\n",
    "            for x,y in L.obsm['spatial']:\n",
    "                values.append([x, y, i*z_scale, str(i)])\n",
    "        df = pd.DataFrame(values, columns=['x','y','z','slice'])\n",
    "        fig = px.scatter_3d(df, x='x', y='y', z='z',\n",
    "                    color='slice',color_discrete_sequence=self.slice_colors)\n",
    "        fig.update_layout(scene_aspectmode='data')\n",
    "        fig.show()\n",
    "\n",
    "    def center_align_stack(self,center,new_slices):\n",
    "        plt.figure(figsize=(7,7))\n",
    "        pst.plot_slice(center,self.center_color,s=400)\n",
    "        for i in range(len(new_slices)):\n",
    "            pst.plot_slice(new_slices[i],self.slice_colors[i],s=400)\n",
    "\n",
    "        legend_handles = [mpatches.Patch(color=color, label=str(i + 1)) for i, color in enumerate(self.slice_colors)]\n",
    "        plt.legend(handles=legend_handles)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    def center_align_center(self,center,new_slices):\n",
    "        num_slices = len(new_slices)\n",
    "        num_plots = num_slices\n",
    "\n",
    "        num_rows = num_plots // 2 + num_plots % 2\n",
    "        num_cols = 2\n",
    "\n",
    "        fig, axs = plt.subplots(num_rows, num_cols, figsize=(7, 7))\n",
    "        axs = axs.flatten() if num_plots > 1 else [axs]\n",
    "\n",
    "        for i in range(num_slices):\n",
    "            pst.plot_slice(center, self.center_color, ax=axs[i])\n",
    "            pst.plot_slice(new_slices[i], self.slice_colors[i], ax=axs[i])\n",
    "\n",
    "        for j in range(num_slices, len(axs)):\n",
    "            fig.delaxes(axs[j])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def align_slices(slices, use_gpu=True):\n",
    "    alignments = {}\n",
    "    backend = ot.backend.TorchBackend()\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(len(slices) - 1):\n",
    "        pi = pst.pairwise_align(slices[i], slices[i + 1], backend=backend, use_gpu=use_gpu)\n",
    "        alignments[f'pi{i+1}{i+2}'] = pi\n",
    "\n",
    "    print('Runtime: ' + str(time.time() - start))\n",
    "    return alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_leiden_cluster(slices, n_top_genes=2000, n_pcs=50):\n",
    "    processed_slices = [slice.copy() for slice in slices]\n",
    "\n",
    "    for slice in processed_slices:\n",
    "        sc.pp.normalize_total(slice, target_sum=1e4)\n",
    "        sc.pp.log1p(slice)\n",
    "        sc.pp.highly_variable_genes(slice, n_top_genes=n_top_genes)\n",
    "        sc.pp.pca(slice, n_comps=n_pcs)\n",
    "        sc.pp.neighbors(slice)\n",
    "        sc.tl.leiden(slice)\n",
    "        slice.obs['layer_guess_reordered'] = slice.obs['leiden']\n",
    "\n",
    "    return processed_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cluster_labels_to_original_slices(original_slices, processed_slices):\n",
    "    for original_slice, processed_slice in zip(original_slices, processed_slices):\n",
    "        original_slice.obs['layer_guess_reordered'] = processed_slice.obs['layer_guess_reordered']\n",
    "    return original_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aligned_slices_with_label(slices):\n",
    "    # processed_slices = generate_leiden_cluster(slices)\n",
    "    # original_slices_with_label = add_cluster_labels_to_original_slices(slices,processed_slices)\n",
    "    alignments = align_slices(slices)\n",
    "    pis = [alignments[f'pi{i}{i+1}'] for i in range(1, len(slices))]\n",
    "    new_slices = pst.stack_slices_pairwise(slices, pis)\n",
    "    return  pis,new_slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric 1 PAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_matrix(slice, n_categories):\n",
    "    binary_matrix = np.zeros((slice.n_obs, n_categories))\n",
    "    for idx, cat in enumerate(slice.obs['layer_guess_reordered']):\n",
    "        binary_matrix[idx, cat - 1] = 1  \n",
    "    return binary_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./../../example_dataset/spatial/\"\n",
    "file_names = [\"151507_preprocessed.h5\", \"151508_preprocessed.h5\"]\n",
    "slices = load_slices_h5(data_dir, file_names)\n",
    "mapping_dict = {'Layer1':1, 'Layer2':2, 'Layer3':3, 'Layer4':4, 'Layer5':5, 'Layer6':6, 'WM':7}\n",
    "backend = ot.backend.TorchBackend()\n",
    "use_gpu=True\n",
    "for slice_i in slices:\n",
    "    slice_i.obs['layer_guess_reordered'] = slice_i.obs['layer_guess_reordered'].map(mapping_dict)\n",
    "    \n",
    "total_accuracy = 0\n",
    "n_categories = len(mapping_dict)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu is available, using gpu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcl/mambaforge/envs/eggplant/lib/python3.8/site-packages/ot/lp/__init__.py:354: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu is available, using gpu.\n",
      "0.27976604027372787\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(slices)):\n",
    "    for j in range(len(slices)):\n",
    "        if i != j:\n",
    "            binary_matrix_i = create_binary_matrix(slices[i], n_categories)\n",
    "            binary_matrix_j = create_binary_matrix(slices[j], n_categories)\n",
    "            pi = pst.pairwise_align(slices[i], slices[j], backend=backend, use_gpu=use_gpu)\n",
    "            matched_pairs = np.dot(binary_matrix_i, binary_matrix_j.T)\n",
    "            total_accuracy += np.sum(pi * matched_pairs)\n",
    "\n",
    "ave_accuracy = total_accuracy / (len(slices) * (len(slices) - 1))\n",
    "\n",
    "print(ave_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics2 Spatial Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(adata, degree = 4):\n",
    "        \"\"\"\n",
    "        Converts spatial coordinates into graph using networkx library.\n",
    "        \n",
    "        param: adata - ST Slice \n",
    "        param: degree - number of edges per vertex\n",
    "\n",
    "        return: 1) G - networkx graph\n",
    "                2) node_dict - dictionary mapping nodes to spots\n",
    "        \"\"\"\n",
    "        D = distance_matrix(adata.obsm['spatial'], adata.obsm['spatial'])\n",
    "        # Get column indexes of the degree+1 lowest values per row\n",
    "        idx = np.argsort(D, 1)[:, 0:degree+1]\n",
    "        # Remove first column since it results in self loops\n",
    "        idx = idx[:, 1:]\n",
    "\n",
    "        G = nx.Graph()\n",
    "        for r in range(len(idx)):\n",
    "            for c in idx[r]:\n",
    "                G.add_edge(r, c)\n",
    "\n",
    "        node_dict = dict(zip(range(adata.shape[0]), adata.obs.index))\n",
    "        return G, node_dict\n",
    "    \n",
    "def generate_graph_from_labels(adata, labels_dict):\n",
    "    \"\"\"\n",
    "    Creates and returns the graph and dictionary {node: cluster_label} for specified layer\n",
    "    \"\"\"\n",
    "    \n",
    "    g, node_to_spot = create_graph(adata)\n",
    "    spot_to_cluster = labels_dict\n",
    "\n",
    "    # remove any nodes that are not mapped to a cluster\n",
    "    removed_nodes = []\n",
    "    for node in node_to_spot.keys():\n",
    "        if (node_to_spot[node] not in spot_to_cluster.keys()):\n",
    "            removed_nodes.append(node)\n",
    "\n",
    "    for node in removed_nodes:\n",
    "        del node_to_spot[node]\n",
    "        g.remove_node(node)\n",
    "        \n",
    "    labels = dict(zip(g.nodes(), [spot_to_cluster[node_to_spot[node]] for node in g.nodes()]))\n",
    "    return g, labels\n",
    "\n",
    "\n",
    "def spatial_entropy(g, labels):\n",
    "    \"\"\"\n",
    "    Calculates spatial entropy of graph  \n",
    "    \"\"\"\n",
    "    # construct contiguity matrix C which counts pairs of cluster edges\n",
    "    cluster_names = np.unique(list(labels.values()))\n",
    "    C = pd.DataFrame(0,index=cluster_names, columns=cluster_names)\n",
    "\n",
    "    for e in g.edges():\n",
    "        C[labels[e[0]]][labels[e[1]]] += 1\n",
    "\n",
    "    # calculate entropy from C\n",
    "    C_sum = C.values.sum()\n",
    "    H = 0\n",
    "    for i in range(len(cluster_names)):\n",
    "        for j in range(i, len(cluster_names)):\n",
    "            if (i == j):\n",
    "                z = C[cluster_names[i]][cluster_names[j]]\n",
    "            else:\n",
    "                z = C[cluster_names[i]][cluster_names[j]] + C[cluster_names[j]][cluster_names[i]]\n",
    "            if z != 0:\n",
    "                H += -(z/C_sum)*math.log(z/C_sum)\n",
    "    return H\n",
    "\n",
    "\n",
    "\n",
    "def spatial_coherence_score(graph, labels):\n",
    "    g, l = graph, labels\n",
    "    true_entropy = spatial_entropy(g, l)\n",
    "    entropies = []\n",
    "    for i in range(100):\n",
    "        new_l = list(l.values())\n",
    "        random.shuffle(new_l)\n",
    "        labels = dict(zip(l.keys(), new_l))\n",
    "        entropies.append(spatial_entropy(g, labels))\n",
    "        \n",
    "    return abs((true_entropy - np.mean(entropies))/np.std(entropies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_spatial_coherence_score(aligned_slices):\n",
    "    total_score = 0\n",
    "    num_slices = len(aligned_slices)\n",
    "    for aligned_slice in aligned_slices:\n",
    "        labels_dict = dict(zip(aligned_slice.obs.index, aligned_slice.obs['layer_guess_reordered']))\n",
    "        g, labels = generate_graph_from_labels(aligned_slice, labels_dict)\n",
    "\n",
    "        score = spatial_coherence_score(g, labels)\n",
    "        total_score += score\n",
    "\n",
    "    average_spatial_coherence_score = total_score / num_slices\n",
    "    print(\"Average Spatial Coherence Score:\", average_spatial_coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu is available, using gpu.\n",
      "gpu is available, using gpu.\n",
      "gpu is available, using gpu.\n",
      "Runtime: 9.939272403717041\n",
      "Average Spatial Coherence Score: 266.42704117007884\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./../../example_dataset/spatial/\"\n",
    "file_names = [\"151507_preprocessed.h5\", \"151508_preprocessed.h5\"]\n",
    "slices = load_slices_h5(data_dir, file_names)\n",
    "mapping_dict = {'Layer1':1, 'Layer2':2, 'Layer3':3, 'Layer4':4, 'Layer5':5, 'Layer6':6, 'WM':7}\n",
    "for slice_i in slices:\n",
    "    slice_i.obs['layer_guess_reordered'] = slice_i.obs['layer_guess_reordered'].map(mapping_dict)\n",
    "\n",
    "\n",
    "_, aligned_slices = aligned_slices_with_label(slices)\n",
    "average_spatial_coherence_score(aligned_slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics 3  LTARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_ltari(slices, k=1):\n",
    "    average_ltari_all_slices = []\n",
    "    for ref_index in range(len(slices)):\n",
    "        reference_slice = slices[ref_index]\n",
    "        ref_coords = reference_slice.obsm['spatial']\n",
    "        nn_model = NearestNeighbors(n_neighbors=k).fit(ref_coords)\n",
    "        ltari_values = []\n",
    "        for query_index in range(len(slices)):\n",
    "            if query_index != ref_index:\n",
    "                query_slice = slices[query_index]\n",
    "                query_coords = query_slice.obsm['spatial']\n",
    "                _, nearest_indices = nn_model.kneighbors(query_coords)\n",
    "                if k == 1:\n",
    "                    transferred_labels = reference_slice.obs['layer_guess_reordered'].iloc[nearest_indices.flatten()].values\n",
    "                else:\n",
    "                    transferred_labels = np.array([reference_slice.obs['layer_guess_reordered'].iloc[indices].mode()[0] for indices in nearest_indices])\n",
    "                ari = adjusted_rand_score(query_slice.obs['layer_guess_reordered'], transferred_labels)\n",
    "                ltari_values.append(ari)\n",
    "        average_ltari_all_slices.append(np.mean(ltari_values))\n",
    "    final_average_ltari = np.mean(average_ltari_all_slices)\n",
    "    return final_average_ltari\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu is available, using gpu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcl/mambaforge/envs/eggplant/lib/python3.8/site-packages/ot/lp/__init__.py:354: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu is available, using gpu.\n",
      "gpu is available, using gpu.\n",
      "Runtime: 100.76190519332886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6299838940364161"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"./../../example_dataset/spatial/\"\n",
    "file_names = [\"151507_preprocessed.h5\", \"151508_preprocessed.h5\"]\n",
    "slices = load_slices_h5(data_dir, file_names)\n",
    "mapping_dict = {'Layer1':1, 'Layer2':2, 'Layer3':3, 'Layer4':4, 'Layer5':5, 'Layer6':6, 'WM':7}\n",
    "backend = ot.backend.TorchBackend()\n",
    "use_gpu=True\n",
    "for slice_i in slices:\n",
    "    slice_i.obs['layer_guess_reordered'] = slice_i.obs['layer_guess_reordered'].map(mapping_dict)\n",
    "\n",
    "_, aligned_slices = aligned_slices_with_label(slices)\n",
    "compute_average_ltari(aligned_slices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
